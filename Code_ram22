import os
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

#  Avantage : lit chaque CSV par morceaux (chunks) et écrit immédiatement sur disque en Parquet.
# On ne garde jamais tout en mémoire → pas de pd.concat géant → plus de MemoryError.

dossier = r"02_resultats_commune_agregation"   # dossier des CSV
fichier_sortie = "mon_dataframe.parquet"       # fichier Parquet final

writer = None
cols = None

for f in sorted(os.listdir(dossier)):
    if not f.endswith(".csv"):
        continue
    chemin = os.path.join(dossier, f)
    for chunk in pd.read_csv(chemin, chunksize=1_000_000, low_memory=True):
        # Normalise le schéma si les colonnes varient légèrement entre fichiers
        if writer is None:
            cols = chunk.columns.tolist()
            table = pa.Table.from_pandas(chunk, preserve_index=False)
            writer = pq.ParquetWriter(fichier_sortie, table.schema)
        else:
            # Aligne sur les colonnes initiales (ajoute/retire au besoin)
            chunk = chunk.reindex(columns=cols)
            table = pa.Table.from_pandas(chunk, preserve_index=False)

        writer.write_table(table)

if writer is not None:
    writer.close()

print(" Terminé :", fichier_sortie)
