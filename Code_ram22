import os
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

# üöÄ AVANTAGE DE CE CODE :
# Ce script permet de parcourir un dossier de CSV volumineux et de les concat√©ner
# SANS saturer la RAM, car :
# 1. On lit les fichiers par morceaux (chunksize) ‚Üí jamais tout en m√©moire
# 2. Chaque morceau est √©crit directement en Parquet ‚Üí pas de concat√©nation g√©ante
# 3. On r√©duit la taille m√©moire avec des types compacts (int32, float32, category)
# R√©sultat : on peut traiter des dizaines de millions de lignes sur une machine standard

# üìÇ Dossier contenant les CSV
dossier = r"02_resultats_commune_agregation"

# üìå Liste des fichiers CSV
fichiers = [f for f in os.listdir(dossier) if f.endswith(".csv")]

# üì¶ Nom du fichier final Parquet
fichier_sortie = "mon_dataframe.parquet"

# Writer parquet (√©criture incr√©mentale)
writer = None

for f in fichiers:
    chemin = os.path.join(dossier, f)
    print(f"Traitement du fichier : {f}")
    
    # Lecture en CHUNKS pour ne pas saturer la RAM
    for chunk in pd.read_csv(
            chemin,
            chunksize=1_000_000,   # ajuste selon ta RAM (ex. 100_000 si tu veux encore + s√ªr)
            usecols=['col1', 'col2', 'col3', 'CPOSTMA', 'date'],  # ‚ö†Ô∏è adapte aux colonnes r√©elles
            dtype={'col1': 'int32', 'col2': 'float32', 'CPOSTMA': 'string'},
            parse_dates=['date'],
            low_memory=True):

        # Ajout d‚Äôinfo temporelle (optionnel)
        chunk['year'] = chunk['date'].dt.year.astype('int16')
        chunk['month'] = chunk['date'].dt.month.astype('int8')

        # Conversion en format Arrow
        table = pa.Table.from_pandas(chunk, preserve_index=False)

        # Cr√©ation du writer √† la premi√®re boucle
        if writer is None:
            writer = pq.ParquetWriter(fichier_sortie, table.schema)

        # √âcriture du chunk dans le fichier Parquet
        writer.write_table(table)

# Fermeture propre
if writer is not None:
    writer.close()

print("‚úÖ Concat√©nation termin√©e. R√©sultat enregistr√© dans :", fichier_sortie)

# V√©rification : relecture rapide
df = pd.read_parquet(fichier_sortie)
print(df.shape)
print(df.head())
