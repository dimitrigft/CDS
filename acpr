from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

----------------------

# DonnÃ©es avec les features sÃ©lectionnÃ©es prÃ©cÃ©demment
X_train_final = X.iloc[train_val_idx][best_row["features"]]
y_train_final = y.iloc[train_val_idx]

# Pipeline complet
pipeline = Pipeline([
    ("smote", SMOTE(random_state=42)),
    ("under", RandomUnderSampler(random_state=42)),
    ("clf", RandomForestClassifier(class_weight="balanced", random_state=42, n_jobs=-1))
])

# Grille d'hyperparamÃ¨tres Ã  tester
param_grid = {
    "clf__n_estimators": [100, 200],
    "clf__max_depth": [None, 10, 20],
    "clf__min_samples_split": [2, 5],
    "clf__min_samples_leaf": [1, 2],
}

------------------------

from sklearn.model_selection import StratifiedKFold

# Cross-validation interne sur le training set
inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

grid = GridSearchCV(
    estimator=pipeline,
    param_grid=param_grid,
    scoring="f1",  # F1-score sur la classe positive
    cv=inner_cv,
    verbose=2,
    n_jobs=-1
)

grid.fit(X_train_final, y_train_final)

----------------------

print("\nâœ… Meilleurs hyperparamÃ¨tres trouvÃ©s :")
print(grid.best_params_)

best_pipeline = grid.best_estimator_

----------------------

# PrÃ©dictions sur le jeu de test
X_test_final = X_test[best_row["features"]]
y_test_final = df_test["SINISTRE_BINARY"]

y_proba_test = best_pipeline.predict_proba(X_test_final)[:, 1]

# ðŸ” Optionnel : rebalayer les seuils pour optimiser le F1
best_thresh = 0.5
best_f1 = 0
for t in np.arange(0.01, 1.0, 0.01):
    y_pred_thresh = (y_proba_test >= t).astype(int)
    score = f1_score(y_test_final, y_pred_thresh)
    if score > best_f1:
        best_f1 = score
        best_thresh = t

# PrÃ©dictions finales avec seuil optimal
y_pred_test = (y_proba_test >= best_thresh).astype(int)

--------------------

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

print("\nðŸ“Š Ã‰valuation finale avec meilleurs hyperparamÃ¨tres + seuil optimal :")
print("Seuil de dÃ©cision optimal :", best_thresh)
print("Confusion Matrix :")
print(confusion_matrix(y_test_final, y_pred_test))
print("\nClassification report :")
print(classification_report(y_test_final, y_pred_test, digits=4))
print(f"AUC : {roc_auc_score(y_test_final, y_proba_test):.4f}")
print(f"F1-score : {f1_score(y_test_final, y_pred_test):.4f}")
