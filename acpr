from sklearn.model_selection import ParameterGrid
from catboost import CatBoostClassifier
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from sklearn.metrics import f1_score, roc_auc_score
import numpy as np
import pandas as pd

# Grille plus riche mais raisonnable
param_grid = {
    'clf__depth': [4, 6, 8],
    'clf__learning_rate': [0.01, 0.05, 0.1],
    'clf__iterations': [100, 200, 300],
    'clf__l2_leaf_reg': [1, 3, 5]
}

grid = list(ParameterGrid(param_grid))
results = []

print(f" Nombre total de combinaisons testées : {len(grid)}")

for i, params in enumerate(grid):
    print(f"\n Test {i+1}/{len(grid)} | Paramètres : {params}")

    pipe = Pipeline([
        ("smote", SMOTE(random_state=42)),
        ("under", RandomUnderSampler(random_state=42)),
        ("clf", CatBoostClassifier(verbose=0, random_state=42))
    ])
    
    pipe.set_params(**params)
    pipe.fit(X_train_sel, y_train_sel)
    y_proba = pipe.predict_proba(X_val_sel)[:, 1]

    best_f1 = 0
    best_auc = 0
    best_thresh = 0.5

    for t in np.arange(0.01, 1.0, 0.01):
        y_pred = (y_proba >= t).astype(int)
        f1 = f1_score(y_val_sel, y_pred)
        auc = roc_auc_score(y_val_sel, y_proba)

        if f1 > best_f1:
            best_f1 = f1
            best_auc = auc
            best_thresh = t

    results.append({
        'params': params,
        'best_threshold': best_thresh,
        'best_f1': best_f1,
        'auc_at_best_thresh': best_auc
    })

# Résumé des meilleurs résultats
results_df = pd.DataFrame(results)
results_df = results_df.sort_values(by="best_f1", ascending=False)

print("\n Top 5 des meilleures combinaisons :")
print(results_df.head(5))

