MODELE 1

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedGroupKFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score, f1_score, confusion_matrix

# --- 1. PrÃ©paration des donnÃ©es ---
df = df_all_trim_final.copy()
df["CONTRAT"] = df["CONTRAT"].astype(str)
df["SINISTRE_BINARY"] = df["SINISTRE"].astype(int)

# Stratification par contrat
contracts_df = df.groupby("CONTRAT").agg({"SINISTRE_BINARY": "max"}).reset_index()

train_val_ids, test_ids = train_test_split(
    contracts_df["CONTRAT"], test_size=0.2,
    stratify=contracts_df["SINISTRE_BINARY"], random_state=42
)

df_train_val = df[df["CONTRAT"].isin(train_val_ids)].copy()
df_test = df[df["CONTRAT"].isin(test_ids)].copy()

# --- 2. Variables explicatives ---
X = df_train_val.drop(columns=["SINISTRE", "SINISTRE_BINARY", "TRIM_YEAR", "CONTRAT", "INSEE_COM", "NOMBRE_SINISTRES"])
X = X.loc[:, ~X.columns.duplicated()]
X = X.astype(np.float32)
y = df_train_val["SINISTRE_BINARY"].astype(np.int32)

X_test = df_test.drop(columns=["SINISTRE", "SINISTRE_BINARY", "TRIM_YEAR", "CONTRAT", "INSEE_COM", "NOMBRE_SINISTRES"])
X_test = X_test.loc[:, ~X_test.columns.duplicated()]
X_test = X_test.astype(np.float32)
y_test = df_test["SINISTRE_BINARY"].astype(np.int32)

# --- 3. ModÃ¨le de rÃ©gression logistique ---
model = LogisticRegression(
    penalty='l2',
    solver='liblinear',
    class_weight='balanced',  # pour compenser le dÃ©sÃ©quilibre
    random_state=42
)

model.fit(X, y)

# --- 4. Ã‰valuation sur le jeu de test ---
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

print("ðŸ“Š Ã‰valuation sur le TEST (RÃ©gression logistique simple) :")
print("Confusion Matrix :")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report :")
print(classification_report(y_test, y_pred, digits=4))
print(f"AUC : {roc_auc_score(y_test, y_proba):.4f}")
print(f"F1-score : {f1_score(y_test, y_pred):.4f}")

MODELE 2

from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectFromModel
from sklearn.pipeline import Pipeline
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from sklearn.metrics import classification_report, f1_score, roc_auc_score, confusion_matrix
import numpy as np

# 1. SÃ©lection des variables importantes (par L1)
selector = SelectFromModel(LogisticRegression(penalty="l1", solver="liblinear", random_state=42), threshold="mean")

# 2. Pipeline complet avec Ã©quilibrage
pipeline = ImbPipeline([
    ("feature_selection", selector),
    ("smote", SMOTE(random_state=42)),
    ("under", RandomUnderSampler(random_state=42)),
    ("clf", LogisticRegression(penalty="l2", class_weight="balanced", solver="liblinear", random_state=42))
])

# 3. EntraÃ®nement
pipeline.fit(X_train_final, y_train_final)

# 4. ProbabilitÃ©s pour seuil optimisÃ©
y_val_proba = pipeline.predict_proba(X_test_final)[:, 1]

# Optimisation du seuil
meilleur_seuil = 0.5
meilleur_f1 = 0

for seuil in np.arange(0.01, 1.0, 0.01):
    y_val_pred = (y_val_proba >= seuil).astype(int)
    f1 = f1_score(y_test_final, y_val_pred)
    if f1 > meilleur_f1:
        meilleur_f1 = f1
        meilleur_seuil = seuil

print(f"\nðŸ” Meilleur seuil trouvÃ© : {meilleur_seuil:.2f} avec F1-score : {meilleur_f1:.4f}")

y_test_pred = (y_val_proba >= meilleur_seuil).astype(int)

print("\n Ã‰valuation finale - RÃ©gression logistique optimisÃ©e :")
print("Confusion matrix :")
print(confusion_matrix(y_test_final, y_test_pred))
print("\nClassification report :")
print(classification_report(y_test_final, y_test_pred, digits=4))
print(f"AUC : {roc_auc_score(y_test_final, y_val_proba):.4f}")
print(f"F1-score optimisÃ© : {f1_score(y_test_final, y_test_pred):.4f}")

MODELE 3

from sklearn.model_selection import train_test_split
import numpy as np

df_all_trim_final["CONTRAT"] = df_all_trim_final["CONTRAT"].astype(str)
df_all_trim_final["SINISTRE_BINARY"] = df_all_trim_final["SINISTRE"].astype(int)

# Split stratifiÃ© au niveau contrat
contracts_df = df_all_trim_final.groupby("CONTRAT").agg({"SINISTRE_BINARY": "max"}).reset_index()

train_val_ids, test_ids = train_test_split(
    contracts_df["CONTRAT"],
    test_size=0.2,
    stratify=contracts_df["SINISTRE_BINARY"],
    random_state=42
)

df_train_val = df_all_trim_final[df_all_trim_final["CONTRAT"].isin(train_val_ids)].copy()
df_test = df_all_trim_final[df_all_trim_final["CONTRAT"].isin(test_ids)].copy()
from sklearn.model_selection import StratifiedGroupKFold

X = df_train_val.drop(columns=["SINISTRE", "SINISTRE_BINARY", "TRIM_YEAR", "CONTRAT", "INSEE_COM", "NOMBRE_SINISTRES"])
y = df_train_val["SINISTRE_BINARY"]
groups = df_train_val["CONTRAT"]

sgkf = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=42)
folds = list(sgkf.split(X, y, groups))

# EntraÃ®nement sur folds 0 Ã  2, validation sur fold 3
train_val_idx = np.concatenate([val_idx for _, val_idx in folds[:3]])
val_idx = folds[3][1]
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, roc_auc_score, classification_report, confusion_matrix

results_val = []
models = []

for fold, (train_idx, val_idx_fold) in enumerate(folds):
    print(f"\n Fold {fold + 1}")
    
    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx_fold]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx_fold]

    model = RandomForestClassifier(n_estimators=100, class_weight="balanced", random_state=42, n_jobs=-1)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_val)
    y_proba = model.predict_proba(X_val)[:, 1]

    f1 = f1_score(y_val, y_pred)
    auc = roc_auc_score(y_val, y_proba)

    print("Confusion Matrix :")
    print(confusion_matrix(y_val, y_pred))
    print("\nClassification Report :")
    print(classification_report(y_val, y_pred, digits=4))
    print(f"AUC : {auc:.4f} | F1-score : {f1:.4f}")

    results_val.append({
        'fold': fold + 1,
        'f1': f1,
        'auc': auc
    })
    models.append(model)

MODELE 4 

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedGroupKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score, f1_score, confusion_matrix
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

# 1. DonnÃ©es
df = df_all_trim_final.copy()
df["CONTRAT"] = df["CONTRAT"].astype(str)
df["SINISTRE_BINARY"] = df["SINISTRE"].astype(int)

# 2. Split stratifiÃ© par contrat
contracts_df = df.groupby("CONTRAT").agg({"SINISTRE_BINARY": "max"}).reset_index()
train_val_ids, test_ids = train_test_split(
    contracts_df["CONTRAT"], test_size=0.2,
    stratify=contracts_df["SINISTRE_BINARY"], random_state=42
)

df_train_val = df[df["CONTRAT"].isin(train_val_ids)].copy()
df_test = df[df["CONTRAT"].isin(test_ids)].copy()

# 3. Variables explicatives
X = df_train_val.drop(columns=["SINISTRE", "SINISTRE_BINARY", "TRIM_YEAR", "CONTRAT", "INSEE_COM", "NOMBRE_SINISTRES"])
X = X.astype(np.float32)
y = df_train_val["SINISTRE_BINARY"].astype(np.int32)
groups = df_train_val["CONTRAT"]

# 4. StratifiedGroupKFold
sgkf = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=42)
folds = list(sgkf.split(X, y, groups))
train_val_idx = np.concatenate([val_idx for _, val_idx in folds[:3]])
val_idx = folds[3][1]

# 5. SÃ©lection de variables
def select_top_features_by_importance(model, X, threshold=0.005):
    importances = model.feature_importances_
    return X.columns[importances > threshold].tolist()

seuils_importance = [0.0005, 0.001, 0.002, 0.005, 0.01]
resultats = []

for seuil in seuils_importance:
    X_train_fold = X.iloc[train_val_idx]
    y_train_fold = y.iloc[train_val_idx]
    X_val_fold = X.iloc[val_idx]
    y_val_fold = y.iloc[val_idx]

    base_model = RandomForestClassifier(n_estimators=100, class_weight="balanced", random_state=42, n_jobs=-1)
    base_model.fit(X_train_fold, y_train_fold)

    selected = select_top_features_by_importance(base_model, X_train_fold, seuil)

    X_train_sel = X_train_fold[selected].values.astype(np.float32)
    y_train_sel = y_train_fold.values.astype(np.int32)
    X_val_sel = X_val_fold[selected].values.astype(np.float32)
    y_val_sel = y_val_fold.values.astype(np.int32)

    pipe = Pipeline([
        ("smote", SMOTE(random_state=42)),
        ("under", RandomUnderSampler(random_state=42)),
        ("clf", RandomForestClassifier(n_estimators=100, class_weight="balanced", random_state=42, n_jobs=-1))
    ])
    pipe.fit(X_train_sel, y_train_sel)
    y_val_proba = pipe.predict_proba(X_val_sel)[:, 1]

    best_thresh = 0.5
    best_f1 = 0
    for t in np.arange(0.01, 1.0, 0.01):
        y_pred = (y_val_proba >= t).astype(int)
        f1 = f1_score(y_val_sel, y_pred)
        if f1 > best_f1:
            best_f1 = f1
            best_thresh = t

    resultats.append({
        "seuil_var": seuil,
        "nb_features": len(selected),
        "best_threshold": best_thresh,
        "best_f1": best_f1,
        "features": selected
    })

# 6. RÃ©sultat optimal
results_df = pd.DataFrame(resultats)
best_row = results_df.sort_values(by="best_f1", ascending=False).iloc[0]

# 7. EntraÃ®nement final
X_train_final = X.iloc[train_val_idx][best_row["features"]].values.astype(np.float32)
y_train_final = y.iloc[train_val_idx].values.astype(np.int32)

X_test = df_test.drop(columns=["SINISTRE", "SINISTRE_BINARY", "TRIM_YEAR", "CONTRAT", "INSEE_COM", "NOMBRE_SINISTRES"])
X_test_final = X_test[best_row["features"]].values.astype(np.float32)
y_test_final = df_test["SINISTRE_BINARY"].values.astype(np.int32)

final_pipe = Pipeline([
    ("smote", SMOTE(random_state=42)),
    ("under", RandomUnderSampler(random_state=42)),
    ("clf", RandomForestClassifier(n_estimators=100, class_weight="balanced", random_state=42, n_jobs=-1))
])
final_pipe.fit(X_train_final, y_train_final)

# 8. PrÃ©dictions finales
y_test_proba = final_pipe.predict_proba(X_test_final)[:, 1]
y_test_pred = (y_test_proba >= best_row["best_threshold"]).astype(int)

# 9. Ã‰valuation
print("\n Ã‰valuation finale sur le TEST (Random Forest) :")
print("Confusion Matrix :")
print(confusion_matrix(y_test_final, y_test_pred))
print("\nClassification Report :")
print(classification_report(y_test_final, y_test_pred, digits=4))
print(f"AUC : {roc_auc_score(y_test_final, y_test_proba):.4f}")
print(f"F1-score : {f1_score(y_test_final, y_test_pred):.4f}")

MODELE 5

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedGroupKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score, f1_score, confusion_matrix
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

# 1. DonnÃ©es
df = df_all_trim_final.copy()
df["CONTRAT"] = df["CONTRAT"].astype(str)
df["SINISTRE_BINARY"] = df["SINISTRE"].astype(int)

# 2. Split stratifiÃ© par contrat
contracts_df = df.groupby("CONTRAT").agg({"SINISTRE_BINARY": "max"}).reset_index()
train_val_ids, test_ids = train_test_split(
    contracts_df["CONTRAT"], test_size=0.2,
    stratify=contracts_df["SINISTRE_BINARY"], random_state=42
)

df_train_val = df[df["CONTRAT"].isin(train_val_ids)].copy()
df_test = df[df["CONTRAT"].isin(test_ids)].copy()

# 3. Variables explicatives
X = df_train_val.drop(columns=["SINISTRE", "SINISTRE_BINARY", "TRIM_YEAR", "CONTRAT", "INSEE_COM", "NOMBRE_SINISTRES"])
X = X.astype(np.float32)
y = df_train_val["SINISTRE_BINARY"].astype(np.int32)
groups = df_train_val["CONTRAT"]

# 4. StratifiedGroupKFold
sgkf = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=42)
folds = list(sgkf.split(X, y, groups))
train_val_idx = np.concatenate([val_idx for _, val_idx in folds[:3]])
val_idx = folds[3][1]

# 5. SÃ©lection de variables
def select_top_features_by_importance(model, X, threshold=0.005):
    importances = model.feature_importances_
    return X.columns[importances > threshold].tolist()

seuils_importance = [0.0005, 0.001, 0.002, 0.005, 0.01]
resultats = []

for seuil in seuils_importance:
    X_train_fold = X.iloc[train_val_idx]
    y_train_fold = y.iloc[train_val_idx]
    X_val_fold = X.iloc[val_idx]
    y_val_fold = y.iloc[val_idx]

    base_model = RandomForestClassifier(n_estimators=100, class_weight="balanced", random_state=42, n_jobs=-1)
    base_model.fit(X_train_fold, y_train_fold)

    selected = select_top_features_by_importance(base_model, X_train_fold, seuil)

    X_train_sel = X_train_fold[selected].values.astype(np.float32)
    y_train_sel = y_train_fold.values.astype(np.int32)
    X_val_sel = X_val_fold[selected].values.astype(np.float32)
    y_val_sel = y_val_fold.values.astype(np.int32)

    pipe = Pipeline([
        ("smote", SMOTE(random_state=42)),
        ("under", RandomUnderSampler(random_state=42)),
        ("clf", RandomForestClassifier(n_estimators=100, class_weight="balanced", random_state=42, n_jobs=-1))
    ])
    pipe.fit(X_train_sel, y_train_sel)
    y_val_proba = pipe.predict_proba(X_val_sel)[:, 1]

    best_thresh = 0.5
    best_f1 = 0
    for t in np.arange(0.01, 1.0, 0.01):
        y_pred = (y_val_proba >= t).astype(int)
        f1 = f1_score(y_val_sel, y_pred)
        if f1 > best_f1:
            best_f1 = f1
            best_thresh = t

    resultats.append({
        "seuil_var": seuil,
        "nb_features": len(selected),
        "best_threshold": best_thresh,
        "best_f1": best_f1,
        "features": selected
    })

# 6. RÃ©sultat optimal
results_df = pd.DataFrame(resultats)
best_row = results_df.sort_values(by="best_f1", ascending=False).iloc[0]

# 7. EntraÃ®nement final
X_train_final = X.iloc[train_val_idx][best_row["features"]].values.astype(np.float32)
y_train_final = y.iloc[train_val_idx].values.astype(np.int32)

X_test = df_test.drop(columns=["SINISTRE", "SINISTRE_BINARY", "TRIM_YEAR", "CONTRAT", "INSEE_COM", "NOMBRE_SINISTRES"])
X_test_final = X_test[best_row["features"]].values.astype(np.float32)
y_test_final = df_test["SINISTRE_BINARY"].values.astype(np.int32)

final_pipe = Pipeline([
    ("smote", SMOTE(random_state=42)),
    ("under", RandomUnderSampler(random_state=42)),
    ("clf", RandomForestClassifier(n_estimators=100, class_weight="balanced", random_state=42, n_jobs=-1))
])
final_pipe.fit(X_train_final, y_train_final)

# 8. PrÃ©dictions finales
y_test_proba = final_pipe.predict_proba(X_test_final)[:, 1]
y_test_pred = (y_test_proba >= best_row["best_threshold"]).astype(int)

# 9. Ã‰valuation
print("\n Ã‰valuation finale sur le TEST (Random Forest) :")
print("Confusion Matrix :")
print(confusion_matrix(y_test_final, y_test_pred))
print("\nClassification Report :")
print(classification_report(y_test_final, y_test_pred, digits=4))
print(f"AUC : {roc_auc_score(y_test_final, y_test_proba):.4f}")
print(f"F1-score : {f1_score(y_test_final, y_test_pred):.4f}")

MODELE 6

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedGroupKFold
from sklearn.metrics import classification_report, roc_auc_score, f1_score, confusion_matrix
from lightgbm import LGBMClassifier
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline

# Ã‰tape 1 : PrÃ©paration des donnÃ©es
df = df_all_trim_final.copy()
df["CONTRAT"] = df["CONTRAT"].astype(str)
df["SINISTRE_BINARY"] = df["SINISTRE"].astype(int)

# Split contrat : train_val vs test
contracts_df = df.groupby("CONTRAT").agg({"SINISTRE_BINARY": "max"}).reset_index()
train_val_ids, test_ids = train_test_split(
    contracts_df["CONTRAT"],
    test_size=0.2,
    stratify=contracts_df["SINISTRE_BINARY"],
    random_state=42
)

df_train_val = df[df["CONTRAT"].isin(train_val_ids)]
df_test = df[df["CONTRAT"].isin(test_ids)]

# Ã‰tape 2 : DÃ©finir X, y, groups
X = df_train_val.drop(columns=["SINISTRE", "SINISTRE_BINARY", "TRIM_YEAR", "CONTRAT", "INSEE_COM", "NOMBRE_SINISTRES"])
X = X.loc[:, ~X.columns.duplicated()]  # Supprimer les colonnes dupliquÃ©es
y = df_train_val["SINISTRE_BINARY"]
groups = df_train_val["CONTRAT"]

# Ã‰tape 3 : Cross-validation par contrat
sgkf = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=42)
folds = list(sgkf.split(X, y, groups=groups))
train_val_idx = np.concatenate([val_idx for _, val_idx in folds[:3]])
val_idx = folds[3][1]

# Ã‰tape 4 : SÃ©lection des variables importantes (par seuil dâ€™importance)
def select_top_features_by_importance(model, X, threshold=0.005):
    importances = model.feature_importances_
    return X.columns[importances > threshold].tolist()

seuils_importance = [0.0005, 0.001, 0.002, 0.005, 0.01]
resultats = []

for seuil in seuils_importance:
    print(f"\nðŸ”Ž Seuil importance : {seuil}")

    model_lgbm = LGBMClassifier(random_state=42, n_jobs=-1)
    model_lgbm.fit(X.iloc[train_val_idx], y.iloc[train_val_idx])

    selected_features = select_top_features_by_importance(model_lgbm, X, seuil)

    X_train_sel = X.iloc[train_val_idx][selected_features]
    y_train_sel = y.iloc[train_val_idx]
    X_val_sel = X.iloc[val_idx][selected_features]
    y_val_sel = y.iloc[val_idx]

    pipe = Pipeline([
        ("smote", SMOTE(random_state=42)),
        ("under", RandomUnderSampler(random_state=42)),
        ("clf", LGBMClassifier(random_state=42, n_jobs=-1))
    ])

    pipe.fit(X_train_sel, y_train_sel)
    y_val_proba = pipe.predict_proba(X_val_sel)[:, 1]

    best_f1 = 0
    best_thresh = 0.5
    for t in np.arange(0.01, 1.0, 0.01):
        y_val_pred = (y_val_proba >= t).astype(int)
        f1 = f1_score(y_val_sel, y_val_pred)
        if f1 > best_f1:
            best_f1 = f1
            best_thresh = t

    resultats.append({
        "seuil_var": seuil,
        "n_features": len(selected_features),
        "best_threshold": best_thresh,
        "best_f1": best_f1,
        "features": selected_features
    })

# Ã‰tape 5 : SÃ©lection du meilleur seuil
results_df = pd.DataFrame(resultats)
best_row = results_df.sort_values(by="best_f1", ascending=False).iloc[0]

# Ã‰tape 6 : RÃ©entraÃ®nement + Ã©valuation sur TEST
X_train_final = X.iloc[train_val_idx][best_row["features"]]
y_train_final = y.iloc[train_val_idx]

X_test = df_test.drop(columns=["SINISTRE", "SINISTRE_BINARY", "TRIM_YEAR", "CONTRAT", "INSEE_COM", "NOMBRE_SINISTRES"])
X_test = X_test.loc[:, ~X_test.columns.duplicated()]  # Supprimer les colonnes dupliquÃ©es
X_test_final = X_test[best_row["features"]]
y_test_final = df_test["SINISTRE_BINARY"]

final_pipeline = Pipeline([
    ("smote", SMOTE(random_state=42)),
    ("under", RandomUnderSampler(random_state=42)),
    ("clf", LGBMClassifier(random_state=42, n_jobs=-1))
])

final_pipeline.fit(X_train_final, y_train_final)
y_test_proba = final_pipeline.predict_proba(X_test_final)[:, 1]
y_test_pred = (y_test_proba >= best_row["best_threshold"]).astype(int)

# RÃ©sultats
print("\nðŸ“Š Ã‰valuation finale sur le TEST :")
print("Confusion matrix :")
print(confusion_matrix(y_test_final, y_test_pred))
print("\nClassification report :")
print(classification_report(y_test_final, y_test_pred, digits=4))
print(f"AUC : {roc_auc_score(y_test_final, y_test_proba):.4f}")
print(f"F1-score : {f1_score(y_test_final, y_test_pred):.4f}")
