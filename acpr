from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, classification_report

results_val = []
models = []

for fold, (train_idx, val_idx) in enumerate(folds):
    print(f"\n===== Fold {fold+1} =====")

    # S√©paration des donn√©es
    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    # Mod√®le Random Forest
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=None,
        class_weight='balanced',
        random_state=42,
        n_jobs=-1
    )

    model.fit(X_train, y_train)

    # Pr√©dictions
    y_pred = model.predict(X_val)
    y_proba = model.predict_proba(X_val)[:, 1]

    # Scores
    f1 = f1_score(y_val, y_pred)
    auc = roc_auc_score(y_val, y_proba)

    print("‚û°Ô∏è Confusion Matrix :")
    print(confusion_matrix(y_val, y_pred))
    print("\n‚û°Ô∏è Classification Report :")
    print(classification_report(y_val, y_pred, digits=4))
    print(f"‚úÖ AUC : {auc:.4f} | F1-score : {f1:.4f}")

    results_val.append({
        'fold': fold + 1,
        'f1': f1,
        'auc': auc
    })
    models.append(model)



# Option : prendre le mod√®le avec le meilleur F1-score
best_idx = max(range(len(results_val)), key=lambda i: results_val[i]["f1"])
best_model = models[best_idx]

print(f"\nüß™ √âvaluation finale sur le jeu de test avec le mod√®le du Fold {best_idx + 1}")

# Pr√©diction
y_test_pred = best_model.predict(X_test)
y_test_proba = best_model.predict_proba(X_test)[:, 1]

# Scores
f1_test = f1_score(y_test, y_test_pred)
auc_test = roc_auc_score(y_test, y_test_proba)

print("‚û°Ô∏è Confusion Matrix (Test) :")
print(confusion_matrix(y_test, y_test_pred))
print("\n‚û°Ô∏è Classification Report (Test) :")
print(classification_report(y_test, y_test_pred, digits=4))
print(f"‚úÖ AUC Test : {auc_test:.4f} | F1-score Test : {f1_test:.4f}")
