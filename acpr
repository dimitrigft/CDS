import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedGroupKFold
from xgboost import XGBClassifier
from sklearn.metrics import f1_score, roc_auc_score, classification_report, confusion_matrix
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

# ====== Étape 1 : Préparation des données ======
df = df_all_trim_final.copy()
df["CONTRAT"] = df["CONTRAT"].astype(str)
df["SINISTRE_BINARY"] = df["SINISTRE"].astype(int)

contracts_df = df.groupby("CONTRAT").agg({"SINISTRE_BINARY": "max"}).reset_index()

train_val_ids, test_ids = train_test_split(
    contracts_df["CONTRAT"],
    test_size=0.2,
    stratify=contracts_df["SINISTRE_BINARY"],
    random_state=42
)

df_train_val = df[df["CONTRAT"].isin(train_val_ids)]
df_test = df[df["CONTRAT"].isin(test_ids)]

X = df_train_val.drop(columns=["SINISTRE", "SINISTRE_BINARY", "TRIM_YEAR", "CONTRAT", "INSEE_COM", "NOMBRE_SINISTRES"])
y = df_train_val["SINISTRE_BINARY"]
groups = df_train_val["CONTRAT"]

# ====== Étape 2 : Folds pour validation croisée ======
sgkf = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=42)
folds = list(sgkf.split(X, y, groups=groups))

# ====== Étape 3 : Sélection du meilleur seuil de variable ======
def select_top_features_by_importance(model, X, threshold):
    importances = model.feature_importances_
    return X.columns[importances > threshold].tolist()

seuils = [0.0005, 0.001, 0.002, 0.005, 0.01]
cv_results = []

for seuil in seuils:
    print(f"\n🔍 Test seuil d'importance = {seuil}")
    fold_metrics = []

    for fold, (train_idx, val_idx) in enumerate(folds):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

        # Sélection des features importantes
        base_model = XGBClassifier(use_label_encoder=False, eval_metric="logloss", random_state=42, n_jobs=-1)
        base_model.fit(X_train, y_train)
        selected = select_top_features_by_importance(base_model, X_train, threshold=seuil)

        X_train_sel = X_train[selected]
        X_val_sel = X_val[selected]

        # Pipeline
        pipe = Pipeline([
            ("smote", SMOTE(random_state=42)),
            ("under", RandomUnderSampler(random_state=42)),
            ("clf", XGBClassifier(use_label_encoder=False, eval_metric="logloss", random_state=42, n_jobs=-1))
        ])
        pipe.fit(X_train_sel, y_train)
        y_val_proba = pipe.predict_proba(X_val_sel)[:, 1]

        # Optimisation seuil
        best_f1 = 0
        best_thresh = 0.5
        for t in np.arange(0.01, 1.0, 0.01):
            pred = (y_val_proba >= t).astype(int)
            f1 = f1_score(y_val, pred)
            if f1 > best_f1:
                best_f1 = f1
                best_thresh = t

        auc = roc_auc_score(y_val, y_val_proba)
        fold_metrics.append({"f1": best_f1, "auc": auc, "threshold": best_thresh})

    # Moyenne sur les folds
    avg_f1 = np.mean([m["f1"] for m in fold_metrics])
    avg_auc = np.mean([m["auc"] for m in fold_metrics])
    avg_thresh = np.mean([m["threshold"] for m in fold_metrics])

    cv_results.append({
        "seuil": seuil,
        "f1": avg_f1,
        "auc": avg_auc,
        "threshold": avg_thresh,
        "features": selected
    })

# ====== Étape 4 : Résumé des résultats cross-val et sélection du meilleur seuil ======
cv_df = pd.DataFrame(cv_results)
best_row = cv_df.sort_values(by="f1", ascending=False).iloc[0]
print("\n✅ Meilleur seuil retenu :")
print(best_row)

# ====== Étape 5 : Entraînement final + test ======
X_test = df_test.drop(columns=["SINISTRE", "SINISTRE_BINARY", "TRIM_YEAR", "CONTRAT", "INSEE_COM", "NOMBRE_SINISTRES"])
y_test = df_test["SINISTRE_BINARY"]

X_train_final = X.iloc[np.concatenate([val_idx for _, val_idx in folds])][best_row["features"]]
y_train_final = y.iloc[np.concatenate([val_idx for _, val_idx in folds])]

X_test_final = X_test[best_row["features"]]

final_pipe = Pipeline([
    ("smote", SMOTE(random_state=42)),
    ("under", RandomUnderSampler(random_state=42)),
    ("clf", XGBClassifier(use_label_encoder=False, eval_metric="logloss", random_state=42, n_jobs=-1))
])
final_pipe.fit(X_train_final, y_train_final)

y_test_proba = final_pipe.predict_proba(X_test_final)[:, 1]
y_test_pred = (y_test_proba >= best_row["threshold"]).astype(int)

print("\n📊 Évaluation finale sur le TEST :")
print("Confusion Matrix :")
print(confusion_matrix(y_test, y_test_pred))
print("\nClassification Report :")
print(classification_report(y_test, y_test_pred, digits=4))
print(f"AUC : {roc_auc_score(y_test, y_test_proba):.4f}")
print(f"F1-score : {f1_score(y_test, y_test_pred):.4f}")
